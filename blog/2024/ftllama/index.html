<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Fine-tuning LlaMa 2 | Julia Jose</title> <meta name="author" content="Julia Jose"> <meta name="description" content="Learn how to fine-tune LlaMa 2 for named entity recognition."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://juliaannjose.github.io/blog/2024/ftllama/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Julia </span>Jose</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching</a> </li> <li class="nav-item "> <a class="nav-link" href="/avocation/">avocation</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Fine-tuning LlaMa 2</h1> <p class="post-meta">April 1, 2024</p> <p class="post-tags"> <a href="/blog/2024"> <i class="fas fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/neural-networks"> <i class="fas fa-hashtag fa-sm"></i> neural_networks</a>     ·   <a href="/blog/category/deep-learning"> <i class="fas fa-tag fa-sm"></i> deep_learning</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Meta LlaMa 2 is an open-source large language model trained on publicly available data on the internet.</p> <p>Parameter Size: 7 billion to 70 billion.</p> <p>Architecture: Transformer architecture with certain modifications on context length, use of grouped-query attention, rotary positional embeddings, and so on. (Refer to <a href="https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/" rel="external nofollow noopener" target="_blank">this paper</a> for training details).</p> <p>While a full fine-tune LlaMa 2 is possible, this guide will use concepts such as <a href="https://huggingface.co/docs/peft/en/index" rel="external nofollow noopener" target="_blank">PEFT (Parameter Efficient Fine-Tuning)</a> and <a href="https://arxiv.org/abs/2106.09685" rel="external nofollow noopener" target="_blank">LoRA</a> to more efficeintly fine-tune LlaMa 2. These approaches consider trade-offs in cost and computational resources, aiming to achieve optimal performance with reduced resource requirements.</p> <p>Before we get started, you will need a GPU node to fine-tune LlaMa 2. You can use <a href="brev.dev">brev.dev</a> to get GPU nodes. The code in this guide was developed using 1 NVIDIA A100 80GB GPU on the NYU HPC Cluster. But you can use any GPU type (RTX8000, V100, A10,..) with &gt;= 28 GB memory.</p> <h3 id="dataset">Dataset</h3> <p>The code in this guide uses the <a href="https://github.com/Fraser-Greenlee/datasets/tree/ec34f7bbad7f4c05fc4df1f6216f1b76f596b8ec/datasets/sem_eval_2020_task_11" rel="external nofollow noopener" target="_blank">Propaganda Techniques Corpus (PTC) dataset</a> to fine-tune LlaMa 2 to detect propaganda techniques in news articles. PTC contains phrase-level annotations of propaganda techniques (eg, name-calling, loaded language, flag-waving, etc) in news articles.</p> <p>Download PTC from <a href="https://github.com/davideturco/SemEval2020-Task11/tree/main/datasets" rel="external nofollow noopener" target="_blank">here</a>.</p> <p>We need a pandas dataframe object with each row representing a news article and the corresponding phrase-technique instances in it. For the scope of this guide, we will focus on one technique at a time (this code can be modified to work with multiple technqiues (multi-label multi-class classification setting)).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/ptc_df-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/ptc_df-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/ptc_df-1400.webp"></source> <img src="/assets/img/ptc_df.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Next, we will turn this pandas dataframe into a <a href="https://huggingface.co/docs/datasets/v1.2.0/loading_datasets.html" rel="external nofollow noopener" target="_blank">huggingface dataset.Dataset</a> object.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>finetune_train_dataset = Dataset.from_pandas(train_df)
finetune_dev_dataset = Dataset.from_pandas(dev_df)
</code></pre></div></div> <h3 id="model">Model</h3> <p>Next up, we load the model (with 4-bit quanitzation) using BitsAndBytes.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from accelerate import FullyShardedDataParallelPlugin, Accelerator

base_model_id = "meta-llama/Llama-2-7b-hf"

quant_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16, # torch.bfloat16 if A100, use getattr(torch, "float16") for RTX8000 or V100
    bnb_4bit_use_double_quant=True,
)

model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=quant_config,device_map={"": Accelerator().local_process_index}) 
</code></pre></div></div> <p>Load tokenizer with left padding to get our training data points to be of the same length.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tokenizer = AutoTokenizer.from_pretrained(
    base_model_id,
    padding_side="left",
    add_eos_token=True,  
    add_bos_token=True,  
)
tokenizer.pad_token = tokenizer.eos_token
</code></pre></div></div> <p>Let’s use the following function to add padding (max length should be determined based on your dataset’s length).</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>max_length = 10000

def tokenize(prompt):
    result = tokenizer(
        prompt,
        truncation=True,
        max_length=max_length,
        padding="max_length",
    )
    result["labels"] = result["input_ids"].copy()
    return result
</code></pre></div></div> <p>We will frame the entity recognition problem using the following prompt template.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def generate_and_tokenize_prompt(data_point):
    full_prompt =f"""I want you to perform a data annotation task. In your output, I want you to return a json dictionary with key as phrase and value as technique, depending on whether you think the phrases in the following text uses name-callling. 
A phrase is "name-calling" if you perceive that it is "Labeling the object of the propaganda campaign as either something the target audience fears, hates, finds undesirable or otherwise loves or praises". 
I want you to respond with a json dictionary strictly having the detected phrases as keys and technique (Name-Calling) as value.

### Text:
{data_point["text"]}

### Output:
{data_point["target"]}
"""
    return tokenize(full_prompt)
</code></pre></div></div> <p>Let’s obtain the tokenized dataset.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tokenized_train_dataset = finetune_train_dataset.map(generate_and_tokenize_prompt)
tokenized_val_dataset = finetune_dev_dataset.map(generate_and_tokenize_prompt)
</code></pre></div></div> <h3 id="fine-tuning-the-model-using-lora">Fine-tuning the model using LoRA</h3> <p>The following code will process the quantized model for training.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from peft import prepare_model_for_kbit_training

model.gradient_checkpointing_enable()
model = prepare_model_for_kbit_training(model)
</code></pre></div></div> <p>Next up, we define our QLoRA configuration. The values were selected using <a href="https://github.com/OpenAccess-AI-Collective/axolotl/blob/main/examples/openllama-3b/qlora.yml" rel="external nofollow noopener" target="_blank">this example</a> as reference.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from peft import LoraConfig, get_peft_model

config = LoraConfig(
    r=8,
    lora_alpha=32,
    target_modules=[ # the linear layers that are now trainable
        "q_proj",
        "k_proj",
        "v_proj",
        "o_proj",
        "gate_proj",
        "up_proj",
        "down_proj",
        "lm_head",
    ],
    bias="none",
    lora_dropout=0.05,  # Conventional
    task_type="CAUSAL_LM",
)

model = get_peft_model(model, config)
</code></pre></div></div> <p>OPTIONAL: (Use <a href="https://huggingface.co/docs/accelerate/en/usage_guides/fsdp" rel="external nofollow noopener" target="_blank">Fully Sharded Data Parallel</a> to make training large models faster).</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from accelerate import FullyShardedDataParallelPlugin, Accelerator
from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig

fsdp_plugin = FullyShardedDataParallelPlugin(
    state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),
    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),
)

accelerator = Accelerator(fsdp_plugin=fsdp_plugin)
model = accelerator.prepare_model(model)
</code></pre></div></div> <p>Use <a href="https://wandb.ai/site" rel="external nofollow noopener" target="_blank">weights and biases</a> to track your experiment (&amp; metrics)</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>!pip install -q wandb -U

import wandb, os
wandb.login()

wandb_project = "propaganda-finetune"
if len(wandb_project) &gt; 0:
    os.environ["WANDB_PROJECT"] = wandb_project
</code></pre></div></div> <p>Finally, start the fine-tuning job:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import transformers

project = "propaganda-finetune"
base_model_name = "llama2-7b"
run_name = base_model_name + "-" + project
output_dir = "./" + run_name

tokenizer.pad_token = tokenizer.eos_token

trainer = transformers.Trainer(
    model=model,
    train_dataset=tokenized_train_dataset,
    eval_dataset=tokenized_val_dataset,
    
    args=transformers.TrainingArguments(
        output_dir=output_dir,
        gradient_accumulation_steps=1,
        per_device_train_batch_size = 1,     # reduce this to avoid OOM errors
        num_train_epochs = 3, 
        optim = "paged_adamw_32bit",         # QLoRA uses paged adamw optimizer 
        lr_scheduler_type = "cosine", 
        learning_rate = 0.0002, 
        bf16 = True,                         # set to True on A100; set to false on RTX8000, V100
        # fp16 = True,                       # set to true on RTX8000, V100
        gradient_checkpointing=True,

        logging_steps = 1,                   # log at each step
        logging_dir="./logs", 

        warmup_steps = 5, 
        weight_decay=0.1,
        
        save_strategy="steps",               # save the model checkpoint every logging step
        save_steps=3,                        # save checkpoints at every third step
        evaluation_strategy="steps", 
        eval_steps=3,                        # Evaluate at every third step
        do_eval=True,                
        report_to="wandb",           
        run_name=f"{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}" ),
    
    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),
)

model.config.use_cache = False  
model.config.pretraining_tp = 1
trainer_stats = trainer.train()
</code></pre></div></div> <h3 id="inference">Inference</h3> <p>Restart your kernel and run the following cells for inference.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

base_model_id = "meta-llama/Llama-2-7b-hf"

quant_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16, 
    bnb_4bit_use_double_quant=True,
)

base_model = AutoModelForCausalLM.from_pretrained(
    base_model_id,  # Llama 2 7b, same as before
    quantization_config=quant_config,  # Same quantization config as before
    device_map={"": Accelerator().local_process_index},
    trust_remote_code=True,
)

eval_tokenizer = AutoTokenizer.from_pretrained(
    base_model_id,
    add_bos_token=True,
    trust_remote_code=True,
)
</code></pre></div></div> <p>Load the QLoRA model from a desired checkpoint.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from peft import PeftModel

ft_model = PeftModel.from_pretrained(base_model, "llama2-7b-propaganda-finetune/checkpoint-371")
</code></pre></div></div> <p>Run inference on a test article.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>eval_prompt = f"""I want you to perform a data annotation task. In your output, I want you to return a json dictionary with key as phrase and value as technique, depending on whether you think the phrases in the following text uses loaded language. 
A phrase is "loaded language" if you perceive that it is "Using words or phrases with strong emotional implications to influence an audience". 
I want you to respond with a json dictionary strictly having the detected phrases as keys and technique (Loaded Language) as value.

### Text:
{test_news_article}

### Output:
"""

model_input = eval_tokenizer(eval_prompt, return_tensors="pt").to("cuda")

with torch.no_grad():
    output = eval_tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=500,repetition_penalty=1.5, top_p=0.1,top_k=20)[0], skip_special_tokens=False)
    new_tokens = output.replace(eval_prompt, "")

</code></pre></div></div> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Julia Jose. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-ZQ8YXNLS1G"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-ZQ8YXNLS1G");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>